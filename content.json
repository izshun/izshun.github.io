[{"title":"DeltaPub 社区项目","date":"2020-02-17T15:31:57.438Z","path":"2020/02/17/项目介绍/","text":"DeltaPub 社区项目本文主要关注用哪些技术解决了哪些问题，整理思路系统架构图 3iUkxs.png 整个项目构建于 Spring Boot 之上，Spring MVC 处理请求，MyBatis 访问数据库，Spring Security 管理用户的权限 Spring MVC 拦截器使用场景拦截用户请求，在调用具体方法之前从cookie中获取凭证，构建用户认证的结果，存入存入SecurityContext，以便于Security进行授权，并存入 ThreadLocal，请求完成后对ThreadLocal 和 SecurityContext 中的对象进行清理。 在请求开始时查询登录用户 在本次请求中持有用户数据 在模板视图上显示用户数据 在请求结束时清理用户数据 拦截用户请求，在调用具体方法之前获取用户 IP 和 id 实现数据统计，如统计 UV 与 DAU。 Redis 使用场景二级缓存：Redis 缓存帖子列表数据，当本地缓存查不到时就查询 Redis。 对性能要求高模块，需要高频访问的模块： 关注功能使用了 Redis 的 ZSet 数据结构实现关注列表和被关注列表，将当前时间做为权重，关注时将数据存入 Redis 中，取关时将 Redis 中的数据删除。 点赞功能使用了 Redis 的 Set 和 String 数据结构分别存储实体的赞和用户的赞，实现实时获取点赞状态和数量。 Redis 中的事务是细粒度的，当需要进行增加或删除操作时将事务打开，完成后提交事务。 统计网站 UV 使用了 Redis 的高级数据类型 HyperLogLog，其中日期做 key、IP 做 value 将指定的 IP 计入UV，实现了统计指定日期范围内的 UV。 统计网站 DAU 使用了 Redis 的高级数据类型 Bitmap，其中日期做 key、userId 做 value 将指定的 用户 计入UV，通过 OR 运算实现了统计指定日期范围内的 DAU。 使用 Redis 存储了用户的登录凭证，解决了分布式环境下的 Session 的共享。 Kafka 使用场景当用户进行评论、点赞、关注，触发相应事件，发送到指定主题，事件消费者监听主题，消费消息发送站内通知。 当用户进行发帖和删帖时，发送异步消息，事件消费者监听到消息后，处理事件，包括检验格式、记录日志、对 Elasticsearch 数据库中的数据进行增删。 当用户进行分享时，发送异步消息，将上传文件与分享功能解耦，实现异步生成长图。 Elasticsearch 分布式搜索引擎使用场景对帖子全文搜索 使用 Elasticsearch，对帖子进行了保存，搜索时根据关键字对帖子的标题和内容检索，并设置了排序规则和高亮显示匹配到的关键词。 多线程与定时任务使用场景ThreadLocal实现线程隔离为什么用 ThreadLocal，每个浏览器访问服务器时，服务器会创建独立的线程进行处理请求，服务器处于多线程的环境，存储用户信息时需要考虑多线程的情况，为了防止并发时产生冲突，将用户信息存入 ThreadLocal 实现线程隔离。 Quartz 分布式定时任务框架使用 Quartz 做定时任务的原因，防止在分布式环境下，不同的服务器做同样的任务产生冲突，而 Quartz 是分布式定时任务框架，数据存储在数据库中，可以实现分布式环境下的定时任务。 业务中使用 Quartz 定时刷新帖子分数，用户后续热度排名。 本地定时任务 Spring 定时任务业务中向云服务器上传图片采用了 ThreadPoolTaskScheduler，启用定时器,监视该图片，一旦生成了，则上传至云服务器。 Spring AOP 和 ControllerAdvice 使用场景ControllerAdvice 实现全局异常处理。 Spring AOP 实现统一记录日志。 Spring 事务使用场景对帖子评论时，需要同步更新帖子的评论数据量，设计到两个实体：评论和帖子，修改了两次数据库，先添加评论，后更新帖子的评论数量。 隔离级别选用 *READ_COMMITTED *解决第二类丢失更新、不可重复读以及幻读。 事务传播行为采用 REQUIRED 支持当前事务(外部事务),如果不存在则创建新事务。 数据结构与算法利用 Trie 实现了发帖、评论时对敏感词过滤。 利用 StackOverflow 的问答排名算法，计算帖子分数，实现帖子热门排名。 项目部署图 3iUtZ6.png","tags":[{"name":"项目","slug":"项目","permalink":"https://izshun.github.io/tags/%E9%A1%B9%E7%9B%AE/"}]},{"title":"基于 Trie 实现敏感词过滤","date":"2020-02-16T13:48:05.944Z","path":"2020/02/16/基于 trie 实现敏感词过滤/","text":"加载外部文件1234567891011121314public void init() &#123; try ( InputStream is = this.getClass().getClassLoader().getResourceAsStream(\"sensitive-words.txt\"); BufferedReader reader = new BufferedReader(new InputStreamReader(is)); ) &#123; String keyword; while ((keyword = reader.readLine()) != null) &#123; // 添加到前缀树 this.addKeyword(keyword); &#125; &#125; catch (IOException e) &#123; logger.error(\"加载敏感词文件失败: \" + e.getMessage()); &#125; &#125; 定义 Trie12345678910111213141516171819202122232425private class TrieNode &#123; // 关键词结束标识 private boolean isKeywordEnd = false; // 子节点(key是下级字符,value是下级节点) private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); public boolean isKeywordEnd() &#123; return isKeywordEnd; &#125; public void setKeywordEnd(boolean keywordEnd) &#123; isKeywordEnd = keywordEnd; &#125; // 添加子节点 public void addSubNode(Character c, TrieNode node) &#123; subNodes.put(c, node); &#125; // 获取子节点 public TrieNode getSubNode(Character c) &#123; return subNodes.get(c); &#125;&#125; 将一个敏感词添加到前缀树中123456789101112131415161718192021private void addKeyword(TrieNode rootNode, String keyword) &#123; TrieNode tempNode = rootNode; for (int i = 0; i &lt; keyword.length(); i++) &#123; char c = keyword.charAt(i); TrieNode subNode = tempNode.getSubNode(c); if (subNode == null) &#123; // 初始化子节点 subNode = new TrieNode(); tempNode.addSubNode(c, subNode); &#125; // 指向子节点,进入下一轮循环 tempNode = subNode; // 设置结束标识 if (i == keyword.length() - 1) &#123; tempNode.setKeywordEnd(true); &#125; &#125; &#125; 过滤敏感词1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public String filter(String text) &#123; if (StringUtils.isBlank(text)) &#123; return null; &#125; // 指针1 TrieNode tempNode = rootNode; // 指针2 int begin = 0; // 指针3 int position = 0; // 结果 StringBuilder sb = new StringBuilder(); while (position &lt; text.length()) &#123; char c = text.charAt(position); // 跳过符号 if (isSymbol(c)) &#123; // 若指针1处于根节点,将此符号计入结果,让指针2向下走一步 if (tempNode == rootNode) &#123; sb.append(c); begin++; &#125; // 无论符号在开头或中间,指针3都向下走一步 position++; continue; &#125; // 检查下级节点 tempNode = tempNode.getSubNode(c); if (tempNode == null) &#123; // 以begin开头的字符串不是敏感词 sb.append(text.charAt(begin)); // 进入下一个位置 position = ++begin; // 重新指向根节点 tempNode = rootNode; &#125; else if (tempNode.isKeywordEnd()) &#123; // 发现敏感词,将begin~position字符串替换掉 sb.append(\"****\"); // 进入下一个位置 begin = ++position; // 重新指向根节点 tempNode = rootNode; &#125; else &#123; // 检查下一个字符 position++; &#125; &#125; // 将最后一批字符计入结果 sb.append(text.substring(begin)); return sb.toString(); &#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://izshun.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Redis 中数据的过期策略和淘汰策略","date":"2020-02-16T13:04:51.572Z","path":"2020/02/16/Redis 中数据的过期策略和淘汰策略/","text":"过期策略Redis 会把设置了过期时间的 key 放入一个独立的字典里，在 key 过期时并不会立刻删除它。 Redis 会通过如下两种策略，来删除过期的key: 惰性删除 客户端访问某个 key 时，Redis 会检查该 key 是否过期，若过期则删除 定期扫描 Redis 默认每秒执行 10 次过期扫描（配置hz选项），扫描策略如下：1. 从过期字典中随机选择 20 个 key； 2. 删除这 20 个 key 中已过期的 key； 3. 如果过期的 key 的比例超过 25%，则重复步骤 1； 淘汰策略当 Redis 占用内存超出最大限制（maxmemory）时，可采用如下策略（maxmemory-policy） 让 Redis 淘汰一些数据，以腾出空间继续提供读写服务： noeviction: 对可能导致增大内存的命令返回错误（大多数写命令，DEL除外） volatile-ttl: 在设置了过期时间的 key 中，选择剩余寿命（TTL）最短的 key，将其淘汰 volatile-lru: 在设置了过期时间的 key 中，选择最少使用的 key，将其淘汰 volatile-random: 在设置了过期时间的 key 中，随机选择一些 key，将其淘汰 allkeys-lru: 在所有的 key 中，选择最少使用的 key，将其淘汰 allkeys-random: 在所有的 key 中，随机选择一些 key，将其淘汰 LRU 算法 维护一个链表，用于顺序存储被访问过的key。 在访问数据时，最新访问过的 key 将被移动到表头 即最近访问的 key 在表头，最少访问的 key 在表尾 近似 LRU 算法（Redis） 给每个 key 维护一个时间戳，淘汰时随机采样 5 个key，从中淘汰掉最旧的key。如果还是超出内存限制，则继续随机采样淘汰 优点：比 LRU 算法节约内存，却可以取得非常近似的效果","tags":[{"name":"Redis","slug":"Redis","permalink":"https://izshun.github.io/tags/Redis/"}]},{"title":"采用 K-Means 聚类算法分析 GPS 轨迹","date":"2020-02-14T02:20:21.451Z","path":"2020/02/14/采用 K-Means 聚类算法分析 GPS 轨迹/","text":"数据集来源Geolife 加载数据12345678910111213141516import numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport osfrom matplotlib.colors import rgb2hexfrom shapely.geometry import MultiPointfrom geopy.distance import great_circlefrom sklearn.cluster import KMeansfrom sklearn.cluster import DBSCANuserdata = 'd:/Lab-work/Geolife Trajectories 1.3/Data/001/Trajectory/'filelist = os.listdir(userdata)names = ['lat','lng','zero','alt','days','date','time']df_list = [pd.read_csv(userdata + f,header=6,names=names,index_col=False) for f in filelist] df = pd.concat(df_list, ignore_index=True)print(df.head(10))plt.plot(df.lat, df.lng) 123456789101112 lat lng zero alt days date time0 39.984198 116.319322 0 492 39744.245208 2008-10-23 05:53:061 39.984224 116.319402 0 492 39744.245266 2008-10-23 05:53:112 39.984211 116.319389 0 492 39744.245324 2008-10-23 05:53:163 39.984217 116.319422 0 491 39744.245382 2008-10-23 05:53:214 39.984710 116.319865 0 320 39744.245405 2008-10-23 05:53:235 39.984674 116.319810 0 325 39744.245463 2008-10-23 05:53:286 39.984623 116.319773 0 326 39744.245521 2008-10-23 05:53:337 39.984606 116.319732 0 327 39744.245579 2008-10-23 05:53:388 39.984555 116.319728 0 324 39744.245637 2008-10-23 05:53:439 39.984579 116.319769 0 309 39744.245694 2008-10-23 05:53:48[&lt;matplotlib.lines.Line2D at 0x17efc43eac8&gt;] 39eBMn.png K-Means123456789coords = df[['lat','lng']].valuesn_clusters = 100cls = KMeans(n_clusters).fit(coords)colors = tuple([(np.random.random(),np.random.random(), np.random.random()) for i in range(n_clusters)])colors = [rgb2hex(x) for x in colors]for i, color in enumerate(colors): members = cls.labels_ == i plt.scatter(coords[members, 0], coords[members, 1], s=60, c=color, alpha=0.5)plt.show() 39mlYF.png 获取 K-Means 聚类结果12345cluster_labels = cls.labels_num_clusters = len(set(cluster_labels) - set([-1]))print('Clustered ' + str(len(df_min)) + ' points to ' + str(num_clusters) + ' clusters')clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])print(clusters) 12345678910111213Clustered 9045 points to 100 clusters0 [[40.014459, 116.305603], [40.014363, 116.3056...1 [[39.975246000000006, 116.358976], [39.975244,...2 [[40.001312, 116.193358], [40.001351, 116.1932...3 [[39.984559000000004, 116.326696], [39.984669,...4 [[39.964969, 116.434923], [39.964886, 116.4350... ... 95 [[40.004549, 116.260581], [40.004515999999995,...96 [[39.97964, 116.323856], [39.979701, 116.32396...97 [[40.0009, 116.23948500000002], [40.000831, 11...98 [[39.962336, 116.32817800000001], [39.96223300...99 [[39.9663, 116.353677], [39.966291999999996, 1...Length: 100, dtype: object 获取每个群集每个中心点12345678def get_centermost_point(cluster): centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y) centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m) return tuple(centermost_point)centermost_points = clusters.map(get_centermost_point)lats, lons = zip(*centermost_points)rep_points = pd.DataFrame(&#123;'lon':lons, 'lat':lats&#125;)print(rep_points) 1234567891011121314 lon lat0 116.306558 40.0137511 116.353295 39.9753572 116.190167 40.0042903 116.326944 39.9864924 116.438241 39.961273.. ... ...95 116.256309 40.00477496 116.326462 39.97875297 116.232672 39.99863098 116.328847 39.95827199 116.358655 39.966451[100 rows x 2 columns] 描绘中心点12345678910111213141516171819fig, ax = plt.subplots(figsize=[10, 6])rs_scatter = ax.scatter(rep_points['lon'][0], rep_points['lat'][0], c='#99cc99', edgecolor='None', alpha=0.7, s=450)ax.scatter(rep_points['lon'][1], rep_points['lat'][1], c='#99cc99', edgecolor='None', alpha=0.7, s=250)ax.scatter(rep_points['lon'][2], rep_points['lat'][2], c='#99cc99', edgecolor='None', alpha=0.7, s=250)ax.scatter(rep_points['lon'][3], rep_points['lat'][3], c='#99cc99', edgecolor='None', alpha=0.7, s=150)df_scatter = ax.scatter(df_min['lng'], df_min['lat'], c='k', alpha=0.9, s=3)ax.set_title('Full GPS trace vs. DBSCAN clusters')ax.set_xlabel('Longitude')ax.set_ylabel('Latitude')ax.legend([df_scatter, rs_scatter], ['GPS points', 'Cluster centers'], loc='upper right')labels = ['cluster&#123;0&#125;'.format(i) for i in range(1, num_clusters+1)]for label, x, y in zip(labels, rep_points['lon'], rep_points['lat']): plt.annotate( label, xy = (x, y), xytext = (-25, -30), textcoords = 'offset points', ha = 'right', va = 'bottom', bbox = dict(boxstyle = 'round,pad=0.5', fc = 'white', alpha = 0.5), arrowprops = dict(arrowstyle = '-&gt;', connectionstyle = 'arc3,rad=0'))plt.show() 39n7CD.png","tags":[{"name":"聚类","slug":"聚类","permalink":"https://izshun.github.io/tags/%E8%81%9A%E7%B1%BB/"}]}]